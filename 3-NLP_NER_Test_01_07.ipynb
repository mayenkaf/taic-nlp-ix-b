{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-NER-Test-01-07.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fw3yWTvZzo8"
      },
      "source": [
        "# Ejercicios de Reconocimiento de Entidad por Nombre (NER)\n",
        "Codigo de Estudiante:....\n",
        "\n",
        "Apellidos y Nombres:.....\n",
        "\n",
        "Fecha de Presentación:...\n",
        "\n",
        "@author: mfch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CzFe-eiZ7uz"
      },
      "source": [
        "## Test 01\n",
        "\n",
        "**Instrucciones**\n",
        "1. Cargue el archivo *uber_apple.txt* en la variable **article**\n",
        "2. Tokenice **article** en oraciones o sentences con **nltk.sent_tokenize**.\n",
        "3. Tokenice cada oración en *sentences* usando una lista comprimida y **nltk.word_token**.\n",
        "4. Dentro de una lista comprimida, etiquete cada oración tokenizada en *partes del discurso* usando **nltk.pos_tag()**.\n",
        "5. Divida cada oración etiquetada en trozos de entidad con nombre usando **nltk.ne_chunk_sents()**. Junto con *pos_sentences*, especifique el argumento de palabra clave adicional **binary = True**.\n",
        "5. Itere sobre cada *sentence* y cada *chunk*, y compruebe si se trata de un *chunk* de entidad con nombre al probar si tiene la etiqueta **label** y si **chunk.label()** es igual a **\"NE\"**. Si es así, imprima ese *chunk*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EinzvZ8PZTKF"
      },
      "source": [
        "with open('/content/uber_apple.txt', mode='r', encoding='utf-8-sig') as f:\n",
        "  article = f.read()\n",
        "article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myKRJu2LadGe"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Tokenize the article into sentences: sentences\n",
        "sentences = ____\n",
        "\n",
        "# Tokenize each sentence into words: token_sentences\n",
        "token_sentences = [____ for sent in ____]\n",
        "\n",
        "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "pos_sentences = [____ for sent in ____] \n",
        "\n",
        "# Create the named entity chunks: chunked_sentences\n",
        "chunked_sentences = ____\n",
        "\n",
        "# Test for stems of the tree with 'NE' tags\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, \"label\") and ____ == \"____\":\n",
        "            print(chunk)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNp1erwGh4-h"
      },
      "source": [
        "## Test 02\n",
        "\n",
        "**Instrucciones**\n",
        "1. Cree un diccionario *defaultdict* llamado **ner_categories**, con el tipo por defecto configurado a **int**\n",
        "2. Llene el diccionario con valores para cada una de las claves **keys**. Recuerde, las claves representaran la etiqueta label(). En el *for* de más afuera itere sobre **chunked_sentences**, usando **sent** como la variable de iteración. En el *for* interno, itere sobre **sent**. Si la condición es verdadera, incremente el valor de cada clave **key** en 1. *Recuerde usar label() por cada chunk*.\n",
        "3. Para las etiquetas de la torta, cree una lista llamada **labels** a partir de las claves de **ner_categories**, que puede ser accedido usando**.keys()**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKrW0tHdNExN"
      },
      "source": [
        "from collections import defaultdict\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences)\n",
        "\n",
        "# Create the defaultdict: ner_categories\n",
        "ner_categories = _____\n",
        "\n",
        "# Create the nested for loop\n",
        "for ____ in ____:\n",
        "    for chunk in ____:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            ____[____.____] += 1\n",
        "            \n",
        "# Create a list from the dictionary keys for the chart labels: labels\n",
        "labels = list(_____)\n",
        "\n",
        "# Create a list of the values: values\n",
        "values = [____.____ for v in labels]\n",
        "\n",
        "# Create the pie chart\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-F7TmmOorWE"
      },
      "source": [
        "## Test 04\n",
        "\n",
        "**Instrucciones**\n",
        "1. Importe spacy.\n",
        "2. Cargue el modelo en inglés 'en' usando spacy.load(). Especifique los argumentos adicionales **tagger=False, parser=False, matcher=False**.\n",
        "3. Cree un objeto *document* **doc** de spacy pasando **article** a  **nlp()**.\n",
        "4. Use **ent** como la variable de iteración, itere sobre las **entities** de **doc** e imprima las etiquetas **ent.label_** y el  texto **ent.text**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpeXFYbkouBG"
      },
      "source": [
        "# Import spacy\n",
        "____\n",
        "\n",
        "# Instantiate the English model: nlp\n",
        "nlp = ____\n",
        "\n",
        "# Create a new document: doc\n",
        "doc = ____\n",
        "\n",
        "# Print all of the found entities and their labels\n",
        "for ____ in ____:\n",
        "    print(____, ____)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPCjrvsraCHo"
      },
      "source": [
        "## Test 06\n",
        "\n",
        "**Instrucciones**\n",
        "1. Instale polyglot, pyicu, Morfessur, pycld2\n",
        "2. Descargue ner2.fr y embbedings.fr; necesarios para usar francés\n",
        "3. Importe la clase **Text** desde polyglot.text\n",
        "4. Usando la cadena en **article** , cree un nuevo objeto **Text** llamado **txt**.\n",
        "5. Itere sobre **txt.entities** e imprimar cada entidad **ent**.\n",
        "6. Imprima el tipo de ent con **type()**.\n",
        "7. Use una lista comprimida para crear una lista de tuplas llamada **entities**. Las expresión de salida de la lista debe ser una tupla, donde el Primer elemento es la etiqueta de la entidad (use el atributo **.tag** de la entidad **ent**); y el segundo elemento es la cadena completa del texto de la entidad (use **.join(ent)**). La variable de iteración debe ser **ent** para iterar sobre **entities** del objeto Text de polyglot llamado **txt**.\n",
        "8. Imprima **entities** para ver lo que se ha creado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5DohzKBdJ7_"
      },
      "source": [
        "!pip3 install polyglot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G54uKRYqezYt"
      },
      "source": [
        "!pip3 install pyicu "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj-HiATnfCTQ"
      },
      "source": [
        "!pip3 install Morfessor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E2w4NmrfHqK"
      },
      "source": [
        "!pip3 install pycld2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtBzpvv7d62k"
      },
      "source": [
        "%%bash\n",
        "polyglot download ner2.fr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlVnprhqfhaT"
      },
      "source": [
        "%%bash\n",
        "polyglot download embeddings2.fr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FpQ7xCTaIib"
      },
      "source": [
        "with open('/content/french.txt', mode='r', encoding='utf-8-sig') as f:\n",
        "  article = f.read()\n",
        "\n",
        "# Importe la clase Text desde polyglot.text\n",
        "from polyglot.text import ____\n",
        "\n",
        "# Create a new text object using Polyglot's Text class: txt\n",
        "txt = ____\n",
        "\n",
        "# Print each of the entities found\n",
        "for ____ in ____:\n",
        "    ____\n",
        "    \n",
        "# Print the type of ent\n",
        "____\n",
        "\n",
        "# Create the list of tuples: entities\n",
        "entities = [(____.____, ' '.____(____)) for ____ in ____.____]\n",
        "\n",
        "# Print entities\n",
        "____\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcPs_-nn7z2l"
      },
      "source": [
        "## Test 07\n",
        "\n",
        "**Instrucciones**\n",
        "1. Itere sobre todas las **entities** de txt, usando **ent** como la variable de iteración.\n",
        "2. Verifique si la entidad contiene *\"Márquez\"* o *\"Gabo\"*. Si es así, incremente el contador **count**. No olvide incluir el acento a \"Márquez\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjeJjELm_a8W"
      },
      "source": [
        "%%bash\n",
        "polyglot download embeddings2.es ner2.es"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZN-jExwAm8i"
      },
      "source": [
        "with open('/content/marquez.txt', mode='r', encoding='utf-8-sig') as f:\n",
        "  article = f.read()\n",
        "\n",
        "article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJrbj_oi96bU"
      },
      "source": [
        "from polyglot.text import Text\n",
        "\n",
        "# Create a new text object using Polyglot's Text class: txt\n",
        "txt = Text(article)\n",
        "# Initialize the count variable: count\n",
        "count = 0\n",
        "\n",
        "# Iterate over all the entities\n",
        "____\n",
        "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
        "    ____\n",
        "        # Increment count\n",
        "        ____\n",
        "\n",
        "# Print count\n",
        "print(\"Entidades Encontradas sobre Márquez o Gabo: \",count)\n",
        "\n",
        "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
        "percentage = count / len(txt.entities)\n",
        "print(\"Porcentaje de las Entidades Máquez o Gabo: \",percentage)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
